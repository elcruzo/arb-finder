{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import xgboost as xgb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('dark_background')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/arbitrage_training_data.csv')\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['is_profitable'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    'spread_binance_coinbase', 'spread_binance_kraken', 'spread_coinbase_kraken',\n",
        "    'volume_binance', 'volume_coinbase', 'volume_kraken',\n",
        "    'volatility', 'hour_of_day', 'day_of_week', 'liquidity_score', 'max_spread_bps'\n",
        "]\n",
        "\n",
        "X = df[feature_cols].values\n",
        "y = df['is_profitable'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xgb_clf = xgb.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42)\n",
        "# xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_clf.fit(X_train, y_train, verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = xgb_clf.predict(X_test)\n",
        "y_prob = xgb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred), roc_auc_score(y_test, y_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame({'feature': feature_cols, 'importance': xgb_clf.feature_importances_}).sort_values('importance', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class ArbitrageNet(nn.Module):\n",
        "#     def __init__(self, input_size):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(input_size, 64)\n",
        "#         self.fc2 = nn.Linear(64, 32)\n",
        "#         self.fc3 = nn.Linear(32, 1)\n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "#         return torch.sigmoid(self.fc3(x))\n",
        "\n",
        "class ArbitrageNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = ArbitrageNet(len(feature_cols)).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_t = torch.FloatTensor(X_train_scaled)\n",
        "y_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "loader = DataLoader(TensorDataset(X_t, y_t), batch_size=512, shuffle=True)\n",
        "\n",
        "# opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train()\n",
        "losses = []\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for bx, by in loader:\n",
        "        bx, by = bx.to(device), by.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = loss_fn(model(bx), by)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss / len(loader))\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'epoch {epoch+1}: {losses[-1]:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xt = torch.FloatTensor(X_test_scaled).to(device)\n",
        "    yp = model(xt).cpu().numpy().flatten()\n",
        "\n",
        "y_pred_nn = (yp > 0.5).astype(int)\n",
        "accuracy_score(y_test, y_pred_nn), f1_score(y_test, y_pred_nn), roc_auc_score(y_test, yp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
        "axes[0].set_title('xgboost')\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_nn), annot=True, fmt='d', ax=axes[1], cmap='Blues')\n",
        "axes[1].set_title('nn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(xgb_clf, models_dir / 'xgboost_classifier.joblib')\n",
        "joblib.dump(scaler, models_dir / 'scaler.joblib')\n",
        "with open(models_dir / 'feature_cols.txt', 'w') as f:\n",
        "    f.write('\\n'.join(feature_cols))\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'input_size': len(feature_cols),\n",
        "    'feature_cols': feature_cols\n",
        "}, models_dir / 'arbitrage_net.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(models_dir.iterdir())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test inference\n",
        "xgb_loaded = joblib.load(models_dir / 'xgboost_classifier.joblib')\n",
        "xgb_loaded.predict_proba(X_test[:5])[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ArbFinder - Arbitrage Analysis & ML Model Training\n",
        "\n",
        "This notebook provides:\n",
        "1. Synthetic data generation for training\n",
        "2. XGBoost model for arbitrage opportunity prediction\n",
        "3. Neural Network model for price spread prediction\n",
        "4. Model evaluation and saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# !pip install pandas numpy scikit-learn xgboost torch matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import joblib\n",
        "\n",
        "# ML imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score, mean_squared_error\n",
        ")\n",
        "\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Neural Network\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"PyTorch not available, will use sklearn MLP instead\")\n",
        "    from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"PyTorch available: {TORCH_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Synthetic Data Generation\n",
        "\n",
        "Generate realistic arbitrage training data with:\n",
        "- Price data from multiple exchanges\n",
        "- Spread calculations\n",
        "- Volume data\n",
        "- Labels for profitable opportunities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_data(n_samples=10000, seed=42):\n",
        "    \"\"\"\n",
        "    Generate synthetic arbitrage data for model training.\n",
        "    \n",
        "    Features:\n",
        "    - price_exchange_a: Price on exchange A\n",
        "    - price_exchange_b: Price on exchange B\n",
        "    - price_exchange_c: Price on exchange C\n",
        "    - spread_ab: Spread between A and B (bps)\n",
        "    - spread_ac: Spread between A and C (bps)\n",
        "    - spread_bc: Spread between B and C (bps)\n",
        "    - volume_a: Volume on exchange A\n",
        "    - volume_b: Volume on exchange B\n",
        "    - volume_c: Volume on exchange C\n",
        "    - volatility: Recent price volatility\n",
        "    - hour_of_day: Hour (0-23)\n",
        "    - day_of_week: Day (0-6)\n",
        "    \n",
        "    Target:\n",
        "    - is_profitable: Binary label (1 if profitable after fees)\n",
        "    - profit_bps: Continuous profit in basis points\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Base price (e.g., BTC around 50000)\n",
        "    base_prices = np.random.uniform(40000, 60000, n_samples)\n",
        "    \n",
        "    # Exchange-specific noise (different exchanges have different prices)\n",
        "    # Exchange A: baseline\n",
        "    price_a = base_prices + np.random.normal(0, 10, n_samples)\n",
        "    \n",
        "    # Exchange B: slightly different, sometimes higher\n",
        "    price_b = base_prices + np.random.normal(5, 15, n_samples)\n",
        "    \n",
        "    # Exchange C: more variance\n",
        "    price_c = base_prices + np.random.normal(-3, 20, n_samples)\n",
        "    \n",
        "    # Calculate spreads in basis points\n",
        "    spread_ab = ((price_b - price_a) / price_a) * 10000\n",
        "    spread_ac = ((price_c - price_a) / price_a) * 10000\n",
        "    spread_bc = ((price_c - price_b) / price_b) * 10000\n",
        "    \n",
        "    # Volume (in quote currency)\n",
        "    volume_a = np.random.exponential(50000, n_samples)\n",
        "    volume_b = np.random.exponential(40000, n_samples)\n",
        "    volume_c = np.random.exponential(30000, n_samples)\n",
        "    \n",
        "    # Volatility (rolling std of returns, simulated)\n",
        "    volatility = np.random.exponential(0.02, n_samples)\n",
        "    \n",
        "    # Time features\n",
        "    hour_of_day = np.random.randint(0, 24, n_samples)\n",
        "    day_of_week = np.random.randint(0, 7, n_samples)\n",
        "    \n",
        "    # Fee structure (in decimal)\n",
        "    fee_a = 0.001  # 0.1%\n",
        "    fee_b = 0.005  # 0.5%\n",
        "    fee_c = 0.0026 # 0.26%\n",
        "    \n",
        "    # Calculate actual profitability\n",
        "    # Best arbitrage: buy low, sell high\n",
        "    max_spread = np.maximum.reduce([spread_ab, spread_ac, spread_bc, -spread_ab, -spread_ac, -spread_bc])\n",
        "    \n",
        "    # Calculate profit after fees (in bps)\n",
        "    # Fees for round trip: buy fee + sell fee\n",
        "    total_fee_ab = (fee_a + fee_b) * 10000  # ~60 bps\n",
        "    total_fee_ac = (fee_a + fee_c) * 10000  # ~36 bps\n",
        "    total_fee_bc = (fee_b + fee_c) * 10000  # ~76 bps\n",
        "    \n",
        "    # Net profit (using minimum fee pair for best case)\n",
        "    min_fee = np.minimum.reduce([total_fee_ab, total_fee_ac, total_fee_bc])\n",
        "    profit_bps = max_spread - min_fee\n",
        "    \n",
        "    # Add some noise to profit (execution slippage, timing, etc.)\n",
        "    profit_bps = profit_bps + np.random.normal(0, 5, n_samples)\n",
        "    \n",
        "    # Profitable if profit > 10 bps threshold\n",
        "    is_profitable = (profit_bps > 10).astype(int)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'price_exchange_a': price_a,\n",
        "        'price_exchange_b': price_b,\n",
        "        'price_exchange_c': price_c,\n",
        "        'spread_ab': spread_ab,\n",
        "        'spread_ac': spread_ac,\n",
        "        'spread_bc': spread_bc,\n",
        "        'volume_a': volume_a,\n",
        "        'volume_b': volume_b,\n",
        "        'volume_c': volume_c,\n",
        "        'volatility': volatility,\n",
        "        'hour_of_day': hour_of_day,\n",
        "        'day_of_week': day_of_week,\n",
        "        'profit_bps': profit_bps,\n",
        "        'is_profitable': is_profitable,\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Generate training data\n",
        "print(\"Generating synthetic training data...\")\n",
        "df = generate_synthetic_data(n_samples=50000)\n",
        "print(f\"Generated {len(df)} samples\")\n",
        "print(f\"Profitable opportunities: {df['is_profitable'].sum()} ({df['is_profitable'].mean()*100:.1f}%)\")\n",
        "print(f\"\\nData shape: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "feature_cols = [\n",
        "    'spread_ab', 'spread_ac', 'spread_bc',\n",
        "    'volume_a', 'volume_b', 'volume_c',\n",
        "    'volatility', 'hour_of_day', 'day_of_week'\n",
        "]\n",
        "\n",
        "X = df[feature_cols].values\n",
        "y_class = df['is_profitable'].values  # For classification\n",
        "y_reg = df['profit_bps'].values       # For regression\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
        "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "_, _, y_train_reg, y_test_reg = train_test_split(\n",
        "    X, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Features: {feature_cols}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. XGBoost Model\n",
        "\n",
        "XGBoost is excellent for tabular data and handles feature interactions well.\n",
        "We train both a classifier (is_profitable) and regressor (profit_bps).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Classifier for profitability prediction\n",
        "print(\"Training XGBoost Classifier...\")\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train_class)\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "y_prob_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n--- XGBoost Classifier Results ---\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test_class, y_pred_xgb):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_class, y_pred_xgb):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test_class, y_pred_xgb):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_test_class, y_pred_xgb):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_test_class, y_prob_xgb):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_xgb, target_names=['Not Profitable', 'Profitable']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Regressor for profit prediction\n",
        "print(\"Training XGBoost Regressor...\")\n",
        "\n",
        "xgb_reg = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_reg.fit(X_train, y_train_reg)\n",
        "\n",
        "# Predictions\n",
        "y_pred_reg = xgb_reg.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = np.mean(np.abs(y_test_reg - y_pred_reg))\n",
        "\n",
        "print(\"\\n--- XGBoost Regressor Results ---\")\n",
        "print(f\"RMSE: {rmse:.4f} bps\")\n",
        "print(f\"MAE:  {mae:.4f} bps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': xgb_clf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance['feature'], importance['importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('../exports/xgboost_feature_importance.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(importance.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Neural Network Model\n",
        "\n",
        "A feedforward neural network for arbitrage prediction.\n",
        "Uses PyTorch if available, otherwise sklearn MLPClassifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n",
        "    # PyTorch Neural Network\n",
        "    class ArbitrageNet(nn.Module):\n",
        "        def __init__(self, input_size, hidden_sizes=[64, 32, 16]):\n",
        "            super(ArbitrageNet, self).__init__()\n",
        "            \n",
        "            layers = []\n",
        "            prev_size = input_size\n",
        "            \n",
        "            for hidden_size in hidden_sizes:\n",
        "                layers.append(nn.Linear(prev_size, hidden_size))\n",
        "                layers.append(nn.ReLU())\n",
        "                layers.append(nn.BatchNorm1d(hidden_size))\n",
        "                layers.append(nn.Dropout(0.2))\n",
        "                prev_size = hidden_size\n",
        "            \n",
        "            layers.append(nn.Linear(prev_size, 1))\n",
        "            layers.append(nn.Sigmoid())\n",
        "            \n",
        "            self.network = nn.Sequential(*layers)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.network(x)\n",
        "    \n",
        "    # Prepare PyTorch data\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "    y_train_tensor = torch.FloatTensor(y_train_class).unsqueeze(1)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "    y_test_tensor = torch.FloatTensor(y_test_class).unsqueeze(1)\n",
        "    \n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = ArbitrageNet(input_size=X_train.shape[1])\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    print(\"Neural Network Architecture:\")\n",
        "    print(model)\n",
        "else:\n",
        "    print(\"Using sklearn MLPClassifier instead of PyTorch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Neural Network\n",
        "if TORCH_AVAILABLE:\n",
        "    print(\"Training PyTorch Neural Network...\")\n",
        "    \n",
        "    epochs = 50\n",
        "    train_losses = []\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_prob_nn = model(X_test_tensor).numpy().flatten()\n",
        "        y_pred_nn = (y_prob_nn > 0.5).astype(int)\n",
        "    \n",
        "    print(\"\\n--- PyTorch Neural Network Results ---\")\n",
        "    print(f\"Accuracy:  {accuracy_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"F1 Score:  {f1_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"ROC AUC:   {roc_auc_score(y_test_class, y_prob_nn):.4f}\")\n",
        "    \n",
        "else:\n",
        "    # Use sklearn MLP\n",
        "    print(\"Training sklearn MLP Classifier...\")\n",
        "    \n",
        "    mlp_clf = MLPClassifier(\n",
        "        hidden_layer_sizes=(64, 32, 16),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=200,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    )\n",
        "    \n",
        "    mlp_clf.fit(X_train_scaled, y_train_class)\n",
        "    \n",
        "    y_pred_nn = mlp_clf.predict(X_test_scaled)\n",
        "    y_prob_nn = mlp_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "    \n",
        "    print(\"\\n--- sklearn MLP Classifier Results ---\")\n",
        "    print(f\"Accuracy:  {accuracy_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"F1 Score:  {f1_score(y_test_class, y_pred_nn):.4f}\")\n",
        "    print(f\"ROC AUC:   {roc_auc_score(y_test_class, y_prob_nn):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Confusion matrices\n",
        "cm_xgb = confusion_matrix(y_test_class, y_pred_xgb)\n",
        "cm_nn = confusion_matrix(y_test_class, y_pred_nn)\n",
        "\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('XGBoost Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
        "axes[1].set_title('Neural Network Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "# Model comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
        "xgb_scores = [\n",
        "    accuracy_score(y_test_class, y_pred_xgb),\n",
        "    precision_score(y_test_class, y_pred_xgb),\n",
        "    recall_score(y_test_class, y_pred_xgb),\n",
        "    f1_score(y_test_class, y_pred_xgb),\n",
        "    roc_auc_score(y_test_class, y_prob_xgb)\n",
        "]\n",
        "nn_scores = [\n",
        "    accuracy_score(y_test_class, y_pred_nn),\n",
        "    precision_score(y_test_class, y_pred_nn),\n",
        "    recall_score(y_test_class, y_pred_nn),\n",
        "    f1_score(y_test_class, y_pred_nn),\n",
        "    roc_auc_score(y_test_class, y_prob_nn)\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "axes[2].bar(x - width/2, xgb_scores, width, label='XGBoost', color='steelblue')\n",
        "axes[2].bar(x + width/2, nn_scores, width, label='Neural Net', color='forestgreen')\n",
        "axes[2].set_ylabel('Score')\n",
        "axes[2].set_title('Model Comparison')\n",
        "axes[2].set_xticks(x)\n",
        "axes[2].set_xticklabels(metrics, rotation=45)\n",
        "axes[2].legend()\n",
        "axes[2].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../exports/model_comparison.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Models\n",
        "\n",
        "Save trained models for deployment:\n",
        "- XGBoost: .joblib format\n",
        "- Neural Network: .pth format (PyTorch) or .joblib (sklearn)\n",
        "- Scaler: for preprocessing new data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save XGBoost models\n",
        "print(\"Saving XGBoost models...\")\n",
        "joblib.dump(xgb_clf, models_dir / 'xgboost_classifier.joblib')\n",
        "joblib.dump(xgb_reg, models_dir / 'xgboost_regressor.joblib')\n",
        "print(f\"  Saved: {models_dir / 'xgboost_classifier.joblib'}\")\n",
        "print(f\"  Saved: {models_dir / 'xgboost_regressor.joblib'}\")\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, models_dir / 'scaler.joblib')\n",
        "print(f\"  Saved: {models_dir / 'scaler.joblib'}\")\n",
        "\n",
        "# Save Neural Network\n",
        "if TORCH_AVAILABLE:\n",
        "    # Save PyTorch model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'input_size': X_train.shape[1],\n",
        "        'hidden_sizes': [64, 32, 16],\n",
        "    }, models_dir / 'neural_net.pth')\n",
        "    print(f\"  Saved: {models_dir / 'neural_net.pth'}\")\n",
        "else:\n",
        "    # Save sklearn MLP\n",
        "    joblib.dump(mlp_clf, models_dir / 'mlp_classifier.joblib')\n",
        "    print(f\"  Saved: {models_dir / 'mlp_classifier.joblib'}\")\n",
        "\n",
        "# Save feature columns for reference\n",
        "with open(models_dir / 'feature_cols.txt', 'w') as f:\n",
        "    f.write('\\n'.join(feature_cols))\n",
        "print(f\"  Saved: {models_dir / 'feature_cols.txt'}\")\n",
        "\n",
        "print(\"\\nAll models saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Loading & Inference Example\n",
        "\n",
        "Demonstrate how to load and use the saved models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load and use models for inference\n",
        "print(\"Loading models for inference...\")\n",
        "\n",
        "# Load scaler\n",
        "loaded_scaler = joblib.load(models_dir / 'scaler.joblib')\n",
        "\n",
        "# Load XGBoost classifier\n",
        "loaded_xgb = joblib.load(models_dir / 'xgboost_classifier.joblib')\n",
        "\n",
        "# Example new data point (simulated arbitrage opportunity)\n",
        "new_data = np.array([[\n",
        "    15.0,   # spread_ab (bps)\n",
        "    -5.0,   # spread_ac (bps)\n",
        "    20.0,   # spread_bc (bps)\n",
        "    45000,  # volume_a\n",
        "    38000,  # volume_b\n",
        "    32000,  # volume_c\n",
        "    0.015,  # volatility\n",
        "    14,     # hour_of_day\n",
        "    2       # day_of_week (Tuesday)\n",
        "]])\n",
        "\n",
        "# Preprocess\n",
        "new_data_scaled = loaded_scaler.transform(new_data)\n",
        "\n",
        "# Predict\n",
        "prediction = loaded_xgb.predict(new_data_scaled)[0]\n",
        "probability = loaded_xgb.predict_proba(new_data_scaled)[0, 1]\n",
        "\n",
        "print(\"\\n--- Inference Example ---\")\n",
        "print(f\"Input features: {new_data[0]}\")\n",
        "print(f\"Prediction: {'Profitable' if prediction == 1 else 'Not Profitable'}\")\n",
        "print(f\"Probability: {probability:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Models trained:\n",
        "1. **XGBoost Classifier** - Best for fast inference, handles feature interactions well\n",
        "2. **XGBoost Regressor** - Predicts actual profit in basis points\n",
        "3. **Neural Network** - Can capture complex non-linear patterns\n",
        "\n",
        "Files saved in `../models/`:\n",
        "- `xgboost_classifier.joblib` - Classification model\n",
        "- `xgboost_regressor.joblib` - Regression model\n",
        "- `neural_net.pth` or `mlp_classifier.joblib` - Neural network\n",
        "- `scaler.joblib` - Feature scaler\n",
        "- `feature_cols.txt` - List of feature names\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
